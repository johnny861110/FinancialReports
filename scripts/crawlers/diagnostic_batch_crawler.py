#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç°¡åŒ–çš„æ‰¹æ¬¡çˆ¬èŸ²è¨ºæ–·è…³æœ¬
"""

import sys
import json
import time
import re
# import PyPDF2  # æš«æ™‚è¨»è§£ï¼Œä½¿ç”¨æ›¿ä»£æ–¹æ¡ˆ
from pathlib import Path
from datetime import datetime

# æ·»åŠ crawlersç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / 'crawlers'))

from improved_twse_crawler import ImprovedTWSEFinancialCrawler

# === è²¡å ±è§£æžåŠŸèƒ½ ===

def extract_text_from_pdf(pdf_path):
    """å¾žPDFæª”æ¡ˆæå–æ–‡å­—å…§å®¹ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
    try:
        # ç”±æ–¼PyPDF2å¯èƒ½ä¸å¯ç”¨ï¼Œæš«æ™‚è¿”å›žæª”åä¿¡æ¯
        # å¯¦éš›ä½¿ç”¨æ™‚å¯ä»¥å®‰è£: pip install PyPDF2
        return f"PDFæª”æ¡ˆ: {pdf_path.name} (éœ€è¦å®‰è£PyPDF2é€²è¡Œæ–‡å­—æå–)"
    except Exception as e:
        print(f"âŒ PDFè®€å–å¤±æ•—: {e}")
        return ""

def extract_eps(text):
    """æå–æ¯è‚¡ç›ˆé¤˜"""
    match = re.search(r"æ¯è‚¡.*?ç›ˆé¤˜.*?([\d\.]+)", text)
    if match:
        return float(match.group(1))
    return None

def extract_numbers_by_line(text, *keywords, min_value=1000000):
    """æ ¹æ“šé—œéµå­—å¾žè¡Œé¦–æå–æ•¸å­—"""
    for line in text.splitlines():
        line_strip = line.strip()
        for keyword in keywords:
            # åªæŠ“è¡Œé¦–é–‹é ­çš„é—œéµå­—
            if keyword and re.match(rf"^{re.escape(keyword)}", line_strip):
                matches = re.findall(r"[\d,]+", line_strip)
                nums = [int(m.replace(",", "")) for m in matches if len(m.replace(",", "")) > 3]
                nums = [n for n in nums if n >= min_value]
                if nums:
                    return [max(nums)]
    return None

def pick_first(nums):
    """é¸å–ç¬¬ä¸€å€‹æ•¸å­—"""
    return nums[0] if nums and len(nums) > 0 else None

def calc_gross_profit(text):
    """è¨ˆç®—æ¯›åˆ©"""
    gross = extract_numbers_by_line(text, "æ¯›åˆ©", "æ¯›åˆ©ç¸½é¡", "ç‡Ÿæ¥­æ¯›åˆ©", "ç‡Ÿæ¥­æ¯›åˆ©ç¸½é¡")
    if gross:
        return pick_first(gross)
    revenue = extract_numbers_by_line(text, "ç‡Ÿæ¥­æ”¶å…¥", "æ”¶å…¥åˆè¨ˆ")
    cost = extract_numbers_by_line(text, "ç‡Ÿæ¥­æˆæœ¬")
    if revenue and cost:
        return pick_first(revenue) - pick_first(cost)
    return None

def calc_operating_income(text):
    """è¨ˆç®—ç‡Ÿæ¥­åˆ©ç›Š"""
    op = extract_numbers_by_line(text, "ç‡Ÿæ¥­åˆ©ç›Š", "ç‡Ÿæ¥­åˆ©ç›Šç¸½é¡", "ç‡Ÿæ¥­æ·¨åˆ©", "ç‡Ÿæ¥­æ”¶å…¥æ·¨é¡")
    if op:
        return pick_first(op)
    revenue = extract_numbers_by_line(text, "ç‡Ÿæ¥­æ”¶å…¥", "æ”¶å…¥åˆè¨ˆ")
    cost = extract_numbers_by_line(text, "ç‡Ÿæ¥­æˆæœ¬")
    expense = extract_numbers_by_line(text, "ç‡Ÿæ¥­è²»ç”¨", "ç‡Ÿæ¥­æ”¯å‡º")
    if revenue and cost and expense:
        return pick_first(revenue) - pick_first(cost) - pick_first(expense)
    return None

def extract_net_income(text):
    """æå–æ·¨åˆ©"""
    ni = extract_numbers_by_line(text, "æ­¸å±¬äºˆæ¯å…¬å¸æ¥­ä¸»ä¹‹æœ¬æœŸæ·¨åˆ©")
    if ni:
        return pick_first(ni)
    return pick_first(
        extract_numbers_by_line(text, "æœ¬æœŸæ·¨åˆ©", "ç¨…å¾Œæ·¨åˆ©", "ç¨…å¾Œç´”ç›Š", "æœ¬æœŸç´”ç›Š")
    )

def parse_financial_report(pdf_path, stock_code, company_name, year, quarter):
    """è§£æžè²¡å ±PDFä¸¦ç”Ÿæˆæ¨™æº–åŒ–JSONï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰"""
    text = extract_text_from_pdf(pdf_path)
    
    # ç”±æ–¼æš«æ™‚ç„¡æ³•æå–PDFå…§å®¹ï¼Œå‰µå»ºåŸºæœ¬çµæ§‹
    result = {
        "stock_code": stock_code,
        "company_name": company_name,
        "report_year": year,
        "report_season": f"Q{quarter}",
        "currency": "TWD",
        "unit": "åƒå…ƒ",
        "financials": {
            "cash_and_equivalents": None,  # éœ€è¦PDFè§£æž
            "accounts_receivable": None,
            "inventory": None,
            "total_assets": None,
            "total_liabilities": None,
            "equity": None
        },
        "income_statement": {
            "net_revenue": None,  # éœ€è¦PDFè§£æž
            "gross_profit": None,
            "operating_income": None,
            "net_income": None,
            "eps": None
        },
        "metadata": {
            "source": "doc.twse.com.tw",
            "file_name": pdf_path.name,
            "file_path": str(pdf_path),
            "file_size": pdf_path.stat().st_size if pdf_path.exists() else 0,
            "crawled_at": datetime.now().isoformat(),
            "parser_version": "v2.3_basic",
            "note": "éœ€è¦å®‰è£PyPDF2é€²è¡Œå®Œæ•´è²¡å ±æ•¸æ“šè§£æž"
        }
    }
    
    # å¦‚æžœèƒ½å¤ è§£æžPDFå…§å®¹ï¼Œå‰‡é€²è¡Œæ•¸æ“šæå–
    if text and "PDFæª”æ¡ˆ:" not in text:
        eps = extract_eps(text)
        
        # æ›´æ–°è²¡å‹™æ•¸æ“š
        result["financials"].update({
            "cash_and_equivalents": pick_first(
                extract_numbers_by_line(text, "ç¾é‡‘åŠç´„ç•¶ç¾é‡‘", "ç¾é‡‘åŠéŠ€è¡Œå­˜æ¬¾")
            ),
            "accounts_receivable": pick_first(
                extract_numbers_by_line(text, "æ‡‰æ”¶å¸³æ¬¾", "æ‡‰æ”¶å¸³æ¬¾ï¼æ·¨é¡", "æ‡‰æ”¶ç¥¨æ“šåŠå¸³æ¬¾")
            ),
            "inventory": pick_first(
                extract_numbers_by_line(text, "å­˜è²¨", "å­˜è²¨ï¼æ·¨é¡", "å­˜  è²¨")
            ),
            "total_assets": pick_first(
                extract_numbers_by_line(text, "è³‡ç”¢ç¸½é¡", "è³‡ç”¢åˆè¨ˆ", "è³‡ç”¢ç¸½è¨ˆ")
            ),
            "total_liabilities": pick_first(
                extract_numbers_by_line(text, "è² å‚µç¸½é¡", "è² å‚µåˆè¨ˆ", "è² å‚µç¸½è¨ˆ")
            ),
            "equity": pick_first(
                extract_numbers_by_line(text, "æ¬Šç›Šç¸½é¡", "æ¬Šç›Šåˆè¨ˆ", "æ¬Šç›Šç¸½è¨ˆ")
            )
        })
        
        result["income_statement"].update({
            "net_revenue": pick_first(
                extract_numbers_by_line(text, "ç‡Ÿæ¥­æ”¶å…¥", "æ”¶å…¥åˆè¨ˆ")
            ),
            "gross_profit": calc_gross_profit(text),
            "operating_income": calc_operating_income(text),
            "net_income": extract_net_income(text),
            "eps": eps
        })
        
        result["metadata"]["parser_version"] = "v2.3_full"
        result["metadata"]["note"] = "å®Œæ•´è²¡å ±æ•¸æ“šè§£æž"
    
    return result

def create_search_index(parsed_reports):
    """å‰µå»ºæœå°‹ç´¢å¼•JSONæª”æ¡ˆ"""
    
    # æŒ‰å…¬å¸å’ŒæœŸé–“çµ„ç¹”è³‡æ–™
    companies = {}
    
    for report in parsed_reports:
        stock_code = report['stock_code']
        company_name = report['company_name']
        period = f"{report['report_year']}Q{report['report_season'][-1]}"
        
        if stock_code not in companies:
            companies[stock_code] = {
                'stock_code': stock_code,
                'company_name': company_name,
                'periods': {}
            }
        
        companies[stock_code]['periods'][period] = {
            'report_year': report['report_year'],
            'report_season': report['report_season'],
            'financials': report['financials'],
            'income_statement': report['income_statement'],
            'metadata': report['metadata']
        }
    
    # å‰µå»ºæœå°‹ç´¢å¼•çµæ§‹
    search_index = {
        'index_info': {
            'created_at': datetime.now().isoformat(),
            'total_companies': len(companies),
            'total_reports': len(parsed_reports),
            'data_source': 'TWSE',
            'index_version': '1.0'
        },
        'companies': companies,
        'summary': {
            'companies_list': [
                {
                    'stock_code': code,
                    'company_name': data['company_name'],
                    'periods_count': len(data['periods']),
                    'available_periods': list(data['periods'].keys())
                }
                for code, data in companies.items()
            ]
        }
    }
    
    return search_index

# === åŽŸæœ‰çš„çˆ¬èŸ²åŠŸèƒ½ ===

def run_diagnostic_batch():
    """é‹è¡Œè¨ºæ–·æ‰¹æ¬¡çˆ¬å–"""
    
    print("ðŸš€ é–‹å§‹è¨ºæ–·æ‰¹æ¬¡çˆ¬å–...")
    
    # ETF 0050éƒ¨åˆ†æˆåˆ†è‚¡ï¼ˆç°¡åŒ–æ¸¬è©¦ï¼‰
    test_stocks = {
        '2330': 'å°ç©é›»',
        '2454': 'è¯ç™¼ç§‘', 
        '2317': 'é´»æµ·'
    }
    
    # æ¸¬è©¦æœŸé–“
    test_periods = [
        (2024, 1),
        (2024, 2)
    ]
    
    # çµ±è¨ˆæ•¸æ“š
    stats = {
        'total_attempts': 0,
        'successful_downloads': 0,
        'failed_downloads': 0,
        'errors': [],
        'parsed_reports': []  # æ–°å¢žï¼šå„²å­˜è§£æžå¾Œçš„è²¡å ±è³‡æ–™
    }
    
    # å‰µå»ºçˆ¬èŸ²
    crawler = ImprovedTWSEFinancialCrawler(max_retries=2, retry_delay=2)
    
    if not crawler.get_initial_page():
        print("âŒ ç„¡æ³•åˆå§‹åŒ–çˆ¬èŸ²")
        return
    
    print("âœ… çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
    
    # è¨­ç½®è¼¸å‡ºç›®éŒ„
    base_dir = Path.cwd() / 'data' / 'diagnostic_results'
    base_dir.mkdir(parents=True, exist_ok=True)
    
    # é–‹å§‹æ‰¹æ¬¡è™•ç†
    total_tasks = len(test_stocks) * len(test_periods)
    current_task = 0
    
    print(f"ðŸ“Š ç¸½ä»»å‹™æ•¸: {total_tasks}")
    print()
    
    for stock_code, company_name in test_stocks.items():
        for year, quarter in test_periods:
            current_task += 1
            
            print(f"ðŸ“‹ [{current_task}/{total_tasks}] è™•ç† {stock_code} ({company_name}) {year}Q{quarter}...")
            
            try:
                # æŸ¥è©¢è²¡å ±
                result = crawler.query_financial_reports(stock_code, year, quarter)
                stats['total_attempts'] += 1
                
                if result and result.get('status') == 'success' and result.get('pdf_files'):
                    print(f"   âœ… æŸ¥è©¢æˆåŠŸï¼Œæ‰¾åˆ° {len(result['pdf_files'])} å€‹PDF")
                    stats['successful_downloads'] += 1
                    
                    # å˜—è©¦ä¸‹è¼‰ç¬¬ä¸€å€‹PDFï¼ˆæ¸¬è©¦ç”¨ï¼‰
                    pdf_info = result['pdf_files'][0]
                    save_dir = base_dir / stock_code / f"{year}Q{quarter}"
                    save_path = save_dir / pdf_info['filename']
                    
                    if crawler.download_pdf_file(pdf_info, save_path):
                        print(f"   âœ… PDFä¸‹è¼‰æˆåŠŸ: {pdf_info['filename']}")
                        
                        # æ–°å¢žï¼šè§£æžPDFå…§å®¹ä¸¦ç”ŸæˆJSON
                        try:
                            parsed_data = parse_financial_report(save_path, stock_code, company_name, year, quarter)
                            if parsed_data:
                                # å„²å­˜å–®ç¨çš„JSONæª”æ¡ˆ
                                json_path = save_path.with_suffix('.json')
                                with open(json_path, 'w', encoding='utf-8') as f:
                                    json.dump(parsed_data, f, ensure_ascii=False, indent=2)
                                
                                stats['parsed_reports'].append(parsed_data)
                                print(f"   âœ… è²¡å ±è§£æžæˆåŠŸ: {json_path.name}")
                            else:
                                print(f"   âš ï¸ è²¡å ±è§£æžå¤±æ•—: ç„¡æ³•æå–å…§å®¹")
                                stats['errors'].append(f"è§£æžå¤±æ•—: {pdf_info['filename']}")
                        except Exception as parse_e:
                            print(f"   âŒ è²¡å ±è§£æžç•°å¸¸: {parse_e}")
                            stats['errors'].append(f"è§£æžç•°å¸¸: {pdf_info['filename']} - {parse_e}")
                    else:
                        print(f"   âŒ PDFä¸‹è¼‰å¤±æ•—: {pdf_info['filename']}")
                        stats['errors'].append(f"PDFä¸‹è¼‰å¤±æ•—: {pdf_info['filename']}")
                
                elif result and result.get('status') in ['financial_holding', 'no_data']:
                    print(f"   âš ï¸ æŸ¥è©¢ç‹€æ…‹: {result['status']} - {result.get('message', '')}")
                    stats['errors'].append(f"æŸ¥è©¢ç‹€æ…‹: {result['status']}")
                else:
                    print(f"   âŒ æŸ¥è©¢å¤±æ•—æˆ–ç‹€æ…‹ç•°å¸¸: {result.get('status') if result else 'None'}")
                    stats['failed_downloads'] += 1
                    stats['errors'].append(f"æŸ¥è©¢å¤±æ•—: {stock_code} {year}Q{quarter}")
                
            except Exception as e:
                print(f"   âŒ è™•ç†ç•°å¸¸: {e}")
                stats['failed_downloads'] += 1
                stats['errors'].append(f"è™•ç†ç•°å¸¸: {stock_code} {year}Q{quarter} - {e}")
            
            # å»¶é²é¿å…é »ç¹è«‹æ±‚
            time.sleep(2.0)
            print()
    
    # é¡¯ç¤ºç¸½çµ
    success_rate = (stats['successful_downloads'] / stats['total_attempts'] * 100) if stats['total_attempts'] > 0 else 0
    
    print(f"ðŸŽ‰ è¨ºæ–·æ‰¹æ¬¡çˆ¬å–å®Œæˆ!")
    print(f"ðŸ“Š çµ±è¨ˆçµæžœ:")
    print(f"   ç¸½å˜—è©¦æ¬¡æ•¸: {stats['total_attempts']}")
    print(f"   æˆåŠŸæŸ¥è©¢: {stats['successful_downloads']}")
    print(f"   å¤±æ•—æŸ¥è©¢: {stats['failed_downloads']}")
    print(f"   è§£æžæˆåŠŸ: {len(stats['parsed_reports'])}")
    print(f"   æˆåŠŸçŽ‡: {success_rate:.1f}%")
    
    # ç”Ÿæˆæœå°‹æª”æ¡ˆJSON
    if stats['parsed_reports']:
        search_index = create_search_index(stats['parsed_reports'])
        search_file = base_dir / f"financial_search_index_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(search_file, 'w', encoding='utf-8') as f:
            json.dump(search_index, f, ensure_ascii=False, indent=2)
        
        print(f"\nðŸ“‹ æœå°‹æª”æ¡ˆå·²ç”Ÿæˆ: {search_file}")
        print(f"   åŒ…å« {len(search_index['companies'])} å®¶å…¬å¸çš„è²¡å ±è³‡æ–™")
    
    if stats['errors']:
        print(f"\nâŒ éŒ¯èª¤åˆ—è¡¨ ({len(stats['errors'])}å€‹):")
        for i, error in enumerate(stats['errors'][:10], 1):  # é¡¯ç¤ºå‰10å€‹éŒ¯èª¤
            print(f"   {i}. {error}")
        
        if len(stats['errors']) > 10:
            print(f"   ... é‚„æœ‰ {len(stats['errors']) - 10} å€‹éŒ¯èª¤")
    
    # å„²å­˜è¨ºæ–·å ±å‘Š
    report = {
        'timestamp': datetime.now().isoformat(),
        'statistics': stats,
        'test_stocks': test_stocks,
        'test_periods': test_periods
    }
    
    report_file = base_dir / f"diagnostic_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    print(f"\nðŸ“„ è¨ºæ–·å ±å‘Šå·²å„²å­˜: {report_file}")

if __name__ == '__main__':
    run_diagnostic_batch()
