#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å°ˆæ¡ˆæ•´ç†èˆ‡å¤§è¦æ¨¡è²¡å ±çˆ¬å–è…³æœ¬
æ¸¬è©¦ç›®æ¨™ï¼š2330ã€2454ã€2317 ä¸‰å®¶å…¬å¸ 2022Q1~2025Q1 å…±39å€‹æœŸé–“
"""

import sys
import json
import time
import shutil
from pathlib import Path
from datetime import datetime

# æ·»åŠ crawlersç›®éŒ„åˆ°Pythonè·¯å¾‘
sys.path.insert(0, str(Path(__file__).parent / 'crawlers'))

from improved_twse_crawler import ImprovedTWSEFinancialCrawler

class ProjectReorganizer:
    """å°ˆæ¡ˆé‡æ–°æ•´ç†å™¨"""
    
    def __init__(self):
        self.root_dir = Path(__file__).parent
        self.data_dir = self.root_dir / 'data'
        self.main_results_dir = self.data_dir / 'financial_reports_main'
        
        # ç›®æ¨™å…¬å¸ï¼ˆ3å®¶é‡é»å…¬å¸ï¼‰
        self.target_companies = {
            '2330': 'å°ç©é›»',
            '2454': 'è¯ç™¼ç§‘',
            '2317': 'é´»æµ·'
        }
        
        # ç›®æ¨™æœŸé–“ï¼ˆ2022Q1~2025Q1ï¼Œå…±13å­£åº¦ï¼‰
        self.target_periods = []
        for year in range(2022, 2026):
            for quarter in range(1, 5):
                if year == 2025 and quarter > 1:  # 2025å¹´åªåˆ°Q1
                    break
                self.target_periods.append((year, quarter))
        
        print(f"ğŸ“Š ç›®æ¨™ï¼š{len(self.target_companies)} å®¶å…¬å¸ Ã— {len(self.target_periods)} å€‹æœŸé–“ = {len(self.target_companies) * len(self.target_periods)} å€‹ä»»å‹™")
        
        # çµ±è¨ˆæ•¸æ“š
        self.stats = {
            'total_attempts': 0,
            'successful_downloads': 0,
            'failed_downloads': 0,
            'no_data_count': 0,
            'financial_holding_count': 0,
            'server_error_count': 0,
            'parsed_reports': [],
            'errors': []
        }
    
    def cleanup_old_data(self):
        """æ¸…ç†èˆŠçš„æ¸¬è©¦è³‡æ–™"""
        print("ğŸ§¹ æ¸…ç†èˆŠçš„æ¸¬è©¦è³‡æ–™...")
        
        cleanup_targets = [
            self.data_dir / 'diagnostic_results',
            self.data_dir / 'etf0050_reports',
            self.root_dir / 'debug_responses',
        ]
        
        for target in cleanup_targets:
            if target.exists():
                try:
                    if target.is_dir():
                        shutil.rmtree(target)
                        print(f"   âœ… å·²åˆªé™¤ç›®éŒ„: {target}")
                    else:
                        target.unlink()
                        print(f"   âœ… å·²åˆªé™¤æª”æ¡ˆ: {target}")
                except Exception as e:
                    print(f"   âš ï¸ ç„¡æ³•åˆªé™¤ {target}: {e}")
    
    def setup_new_structure(self):
        """å»ºç«‹æ–°çš„å°ˆæ¡ˆçµæ§‹"""
        print("ğŸ“ å»ºç«‹æ–°çš„å°ˆæ¡ˆçµæ§‹...")
        
        # ä¸»è¦ç›®éŒ„çµæ§‹
        dirs_to_create = [
            self.main_results_dir,
            self.main_results_dir / 'by_company',
            self.main_results_dir / 'by_period',
            self.main_results_dir / 'reports',
            self.main_results_dir / 'search_indexes',
            self.root_dir / 'debug_responses'
        ]
        
        for dir_path in dirs_to_create:
            dir_path.mkdir(parents=True, exist_ok=True)
            print(f"   âœ… å»ºç«‹ç›®éŒ„: {dir_path}")
        
        # ç‚ºæ¯å®¶å…¬å¸å»ºç«‹ç›®éŒ„
        for stock_code, company_name in self.target_companies.items():
            company_dir = self.main_results_dir / 'by_company' / f"{stock_code}_{company_name}"
            company_dir.mkdir(exist_ok=True)
            print(f"   âœ… å»ºç«‹å…¬å¸ç›®éŒ„: {company_dir}")
        
        # ç‚ºæ¯å€‹æœŸé–“å»ºç«‹ç›®éŒ„
        for year, quarter in self.target_periods:
            period_dir = self.main_results_dir / 'by_period' / f"{year}Q{quarter}"
            period_dir.mkdir(exist_ok=True)
        
        print(f"   âœ… å»ºç«‹ {len(self.target_periods)} å€‹æœŸé–“ç›®éŒ„")

class ComprehensiveCrawler(ProjectReorganizer):
    """å…¨é¢æ€§è²¡å ±çˆ¬èŸ²"""
    
    def __init__(self):
        super().__init__()
        self.crawler = None
    
    def initialize_crawler(self):
        """åˆå§‹åŒ–çˆ¬èŸ²"""
        print("ğŸš€ åˆå§‹åŒ–æ”¹é€²ç‰ˆçˆ¬èŸ²...")
        
        try:
            self.crawler = ImprovedTWSEFinancialCrawler(max_retries=3, retry_delay=3)
            
            if self.crawler.get_initial_page():
                print("âœ… çˆ¬èŸ²åˆå§‹åŒ–æˆåŠŸ")
                return True
            else:
                print("âŒ çˆ¬èŸ²åˆå§‹åŒ–å¤±æ•—")
                return False
                
        except Exception as e:
            print(f"âŒ çˆ¬èŸ²åˆå§‹åŒ–ç•°å¸¸: {e}")
            return False
    
    def crawl_single_report(self, stock_code, company_name, year, quarter, progress_info=""):
        """çˆ¬å–å–®ä¸€è²¡å ±"""
        
        print(f"ğŸ“‹ {progress_info} è™•ç† {stock_code} ({company_name}) {year}Q{quarter}...")
        
        try:
            # æŸ¥è©¢è²¡å ±
            result = self.crawler.query_financial_reports(stock_code, year, quarter)
            self.stats['total_attempts'] += 1
            
            if result and result.get('status') == 'success' and result.get('pdf_files'):
                print(f"   âœ… æŸ¥è©¢æˆåŠŸï¼Œæ‰¾åˆ° {len(result['pdf_files'])} å€‹PDF")
                
                # å»ºç«‹å„²å­˜ç›®éŒ„
                company_dir = self.main_results_dir / 'by_company' / f"{stock_code}_{company_name}"
                period_dir = company_dir / f"{year}Q{quarter}"
                period_dir.mkdir(parents=True, exist_ok=True)
                
                # ä¸‹è¼‰ä¸¦è§£æPDF
                download_success = False
                for pdf_info in result['pdf_files']:
                    save_path = period_dir / pdf_info['filename']
                    
                    if self.crawler.download_pdf_file(pdf_info, save_path):
                        print(f"   âœ… PDFä¸‹è¼‰æˆåŠŸ: {pdf_info['filename']}")
                        download_success = True
                        
                        # è§£æè²¡å ±ä¸¦ç”ŸæˆJSON
                        try:
                            parsed_data = self.parse_financial_report(save_path, stock_code, company_name, year, quarter)
                            if parsed_data:
                                # å„²å­˜å€‹åˆ¥JSONæª”æ¡ˆ
                                json_path = save_path.with_suffix('.json')
                                with open(json_path, 'w', encoding='utf-8') as f:
                                    json.dump(parsed_data, f, ensure_ascii=False, indent=2)
                                
                                self.stats['parsed_reports'].append(parsed_data)
                                print(f"   âœ… è²¡å ±è§£ææˆåŠŸ: {json_path.name}")
                                
                                # è¤‡è£½åˆ°æœŸé–“ç›®éŒ„
                                period_summary_dir = self.main_results_dir / 'by_period' / f"{year}Q{quarter}"
                                period_summary_file = period_summary_dir / f"{stock_code}_{company_name}.json"
                                
                                with open(period_summary_file, 'w', encoding='utf-8') as f:
                                    json.dump(parsed_data, f, ensure_ascii=False, indent=2)
                            else:
                                print(f"   âš ï¸ è²¡å ±è§£æå¤±æ•—")
                                self.stats['errors'].append(f"è§£æå¤±æ•—: {stock_code} {year}Q{quarter}")
                        except Exception as parse_e:
                            print(f"   âŒ è²¡å ±è§£æç•°å¸¸: {parse_e}")
                            self.stats['errors'].append(f"è§£æç•°å¸¸: {stock_code} {year}Q{quarter} - {parse_e}")
                    else:
                        print(f"   âŒ PDFä¸‹è¼‰å¤±æ•—: {pdf_info['filename']}")
                        self.stats['errors'].append(f"ä¸‹è¼‰å¤±æ•—: {pdf_info['filename']}")
                
                if download_success:
                    self.stats['successful_downloads'] += 1
                    return True
                else:
                    self.stats['failed_downloads'] += 1
                    return False
            
            elif result and result.get('status') in ['financial_holding', 'no_data', 'server_error']:
                status = result.get('status')
                message = result.get('message', '')
                print(f"   âš ï¸ æŸ¥è©¢ç‹€æ…‹: {status} - {message}")
                
                if status == 'no_data':
                    self.stats['no_data_count'] += 1
                elif status == 'financial_holding':
                    self.stats['financial_holding_count'] += 1
                elif status == 'server_error':
                    self.stats['server_error_count'] += 1
                
                self.stats['errors'].append(f"æŸ¥è©¢ç‹€æ…‹: {status} - {stock_code} {year}Q{quarter}")
                return False
            else:
                print(f"   âŒ æŸ¥è©¢å¤±æ•—æˆ–ç‹€æ…‹ç•°å¸¸: {result.get('status') if result else 'None'}")
                self.stats['failed_downloads'] += 1
                self.stats['errors'].append(f"æŸ¥è©¢å¤±æ•—: {stock_code} {year}Q{quarter}")
                return False
                
        except Exception as e:
            print(f"   âŒ è™•ç†ç•°å¸¸: {e}")
            self.stats['failed_downloads'] += 1
            self.stats['errors'].append(f"è™•ç†ç•°å¸¸: {stock_code} {year}Q{quarter} - {e}")
            return False
    
    def parse_financial_report(self, pdf_path, stock_code, company_name, year, quarter):
        """è§£æè²¡å ±ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œå¯å¾ŒçºŒæ“´å±•ï¼‰"""
        
        # åŸºæœ¬çš„è²¡å ±JSONçµæ§‹
        result = {
            "stock_code": stock_code,
            "company_name": company_name,
            "report_year": year,
            "report_season": f"Q{quarter}",
            "currency": "TWD",
            "unit": "åƒå…ƒ",
            "financials": {
                "cash_and_equivalents": None,
                "accounts_receivable": None,
                "inventory": None,
                "total_assets": None,
                "total_liabilities": None,
                "equity": None
            },
            "income_statement": {
                "net_revenue": None,
                "gross_profit": None,
                "operating_income": None,
                "net_income": None,
                "eps": None
            },
            "metadata": {
                "source": "doc.twse.com.tw",
                "file_name": pdf_path.name,
                "file_path": str(pdf_path),
                "file_size": pdf_path.stat().st_size if pdf_path.exists() else 0,
                "crawled_at": datetime.now().isoformat(),
                "parser_version": "v3.0_basic",
                "note": "åŸºæœ¬çµæ§‹ï¼Œå¯æ“´å±•PDFå…§å®¹è§£æ"
            }
        }
        
        return result
    
    def run_comprehensive_crawl(self):
        """é‹è¡Œå…¨é¢æ€§çˆ¬å–"""
        
        print("ğŸš€ é–‹å§‹å…¨é¢æ€§è²¡å ±çˆ¬å–...")
        print(f"ğŸ“Š ç›®æ¨™ç¯„åœ: {list(self.target_companies.values())} - {self.target_periods[0][0]}Q{self.target_periods[0][1]} è‡³ {self.target_periods[-1][0]}Q{self.target_periods[-1][1]}")
        
        # åˆå§‹åŒ–çˆ¬èŸ²
        if not self.initialize_crawler():
            print("âŒ ç„¡æ³•åˆå§‹åŒ–çˆ¬èŸ²ï¼Œçµ‚æ­¢åŸ·è¡Œ")
            return False
        
        total_tasks = len(self.target_companies) * len(self.target_periods)
        current_task = 0
        
        # é–‹å§‹æ‰¹æ¬¡çˆ¬å–
        for stock_code, company_name in self.target_companies.items():
            print(f"\nğŸ¢ é–‹å§‹è™•ç† {stock_code} ({company_name})...")
            
            for year, quarter in self.target_periods:
                current_task += 1
                progress = f"[{current_task}/{total_tasks}]"
                
                success = self.crawl_single_report(stock_code, company_name, year, quarter, progress)
                
                # å‹•æ…‹èª¿æ•´å»¶é²
                if success:
                    delay = 2.0  # æˆåŠŸæ™‚è¼ƒçŸ­å»¶é²
                else:
                    delay = 3.0  # å¤±æ•—æ™‚è¼ƒé•·å»¶é²
                
                # æ¯10å€‹ä»»å‹™å¾Œé‡æ–°åˆå§‹åŒ–é€£æ¥
                if current_task % 10 == 0:
                    print(f"ğŸ”„ å·²è™•ç† {current_task} å€‹ä»»å‹™ï¼Œé‡æ–°åˆå§‹åŒ–é€£æ¥...")
                    if not self.initialize_crawler():
                        print("âŒ é‡æ–°åˆå§‹åŒ–å¤±æ•—ï¼Œçµ‚æ­¢åŸ·è¡Œ")
                        break
                    delay += 2.0
                
                time.sleep(delay)
        
        # ç”Ÿæˆç¸½çµå ±å‘Š
        self.generate_comprehensive_report()
        return True
    
    def generate_comprehensive_report(self):
        """ç”Ÿæˆå…¨é¢æ€§ç¸½çµå ±å‘Š"""
        
        print("\nğŸ“‹ ç”Ÿæˆå…¨é¢æ€§ç¸½çµå ±å‘Š...")
        
        # è¨ˆç®—çµ±è¨ˆæ•¸æ“š
        success_rate = (self.stats['successful_downloads'] / self.stats['total_attempts'] * 100) if self.stats['total_attempts'] > 0 else 0
        
        # ä¸»è¦çµ±è¨ˆå ±å‘Š
        main_report = {
            'project_info': {
                'title': 'ETFé‡é»æˆåˆ†è‚¡è²¡å ±å…¨é¢çˆ¬å–',
                'target_companies': self.target_companies,
                'target_periods': self.target_periods,
                'created_at': datetime.now().isoformat()
            },
            'statistics': {
                'total_attempts': self.stats['total_attempts'],
                'successful_downloads': self.stats['successful_downloads'],
                'failed_downloads': self.stats['failed_downloads'],
                'no_data_count': self.stats['no_data_count'],
                'financial_holding_count': self.stats['financial_holding_count'],
                'server_error_count': self.stats['server_error_count'],
                'success_rate': f"{success_rate:.1f}%",
                'parsed_reports_count': len(self.stats['parsed_reports'])
            },
            'coverage_analysis': self.analyze_coverage(),
            'error_summary': self.analyze_errors()
        }
        
        # å„²å­˜ä¸»è¦å ±å‘Š
        main_report_file = self.main_results_dir / 'reports' / f"comprehensive_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(main_report_file, 'w', encoding='utf-8') as f:
            json.dump(main_report, f, ensure_ascii=False, indent=2)
        
        # ç”Ÿæˆæœå°‹ç´¢å¼•
        if self.stats['parsed_reports']:
            search_index = self.create_comprehensive_search_index()
            search_file = self.main_results_dir / 'search_indexes' / f"financial_index_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(search_file, 'w', encoding='utf-8') as f:
                json.dump(search_index, f, ensure_ascii=False, indent=2)
            
            print(f"âœ… æœå°‹ç´¢å¼•å·²ç”Ÿæˆ: {search_file}")
        
        # é¡¯ç¤ºçµæœ
        print(f"\nğŸ‰ å…¨é¢æ€§çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“Š çµ±è¨ˆçµæœ:")
        print(f"   ç¸½å˜—è©¦æ¬¡æ•¸: {self.stats['total_attempts']}")
        print(f"   æˆåŠŸä¸‹è¼‰: {self.stats['successful_downloads']}")
        print(f"   å¤±æ•—æ¬¡æ•¸: {self.stats['failed_downloads']}")
        print(f"   æŸ¥ç„¡è³‡æ–™: {self.stats['no_data_count']}")
        print(f"   æˆåŠŸç‡: {success_rate:.1f}%")
        print(f"   è§£æå ±å‘Šæ•¸: {len(self.stats['parsed_reports'])}")
        
        print(f"\nğŸ“„ å ±å‘Šæª”æ¡ˆ:")
        print(f"   ä¸»è¦å ±å‘Š: {main_report_file}")
        if self.stats['parsed_reports']:
            print(f"   æœå°‹ç´¢å¼•: {search_file}")
        
        if self.stats['errors']:
            print(f"\nâŒ éŒ¯èª¤çµ±è¨ˆ ({len(self.stats['errors'])}):")
            error_types = {}
            for error in self.stats['errors']:
                error_key = error.split(':')[0] if ':' in error else error[:20]
                error_types[error_key] = error_types.get(error_key, 0) + 1
            
            for error_type, count in sorted(error_types.items(), key=lambda x: x[1], reverse=True)[:10]:
                print(f"   {error_type}: {count}")
    
    def analyze_coverage(self):
        """åˆ†æè¦†è“‹ç‡"""
        coverage = {
            'by_company': {},
            'by_period': {},
            'total_expected': len(self.target_companies) * len(self.target_periods)
        }
        
        # æŒ‰å…¬å¸åˆ†æ
        for stock_code, company_name in self.target_companies.items():
            company_reports = [r for r in self.stats['parsed_reports'] if r['stock_code'] == stock_code]
            coverage['by_company'][stock_code] = {
                'company_name': company_name,
                'reports_count': len(company_reports),
                'coverage_rate': f"{len(company_reports) / len(self.target_periods) * 100:.1f}%",
                'available_periods': [f"{r['report_year']}Q{r['report_season'][-1]}" for r in company_reports]
            }
        
        # æŒ‰æœŸé–“åˆ†æ
        for year, quarter in self.target_periods:
            period_key = f"{year}Q{quarter}"
            period_reports = [r for r in self.stats['parsed_reports'] 
                            if r['report_year'] == year and r['report_season'] == f"Q{quarter}"]
            coverage['by_period'][period_key] = {
                'reports_count': len(period_reports),
                'coverage_rate': f"{len(period_reports) / len(self.target_companies) * 100:.1f}%",
                'available_companies': [r['stock_code'] for r in period_reports]
            }
        
        return coverage
    
    def analyze_errors(self):
        """åˆ†æéŒ¯èª¤"""
        error_analysis = {
            'total_errors': len(self.stats['errors']),
            'error_types': {},
            'sample_errors': self.stats['errors'][:20]  # å‰20å€‹éŒ¯èª¤æ¨£æœ¬
        }
        
        for error in self.stats['errors']:
            error_type = error.split(':')[0] if ':' in error else 'unknown'
            error_analysis['error_types'][error_type] = error_analysis['error_types'].get(error_type, 0) + 1
        
        return error_analysis
    
    def create_comprehensive_search_index(self):
        """å‰µå»ºå…¨é¢æ€§æœå°‹ç´¢å¼•"""
        
        # æŒ‰å…¬å¸å’ŒæœŸé–“çµ„ç¹”è³‡æ–™
        companies = {}
        
        for report in self.stats['parsed_reports']:
            stock_code = report['stock_code']
            company_name = report['company_name']
            period = f"{report['report_year']}Q{report['report_season'][-1]}"
            
            if stock_code not in companies:
                companies[stock_code] = {
                    'stock_code': stock_code,
                    'company_name': company_name,
                    'periods': {}
                }
            
            companies[stock_code]['periods'][period] = {
                'report_year': report['report_year'],
                'report_season': report['report_season'],
                'financials': report['financials'],
                'income_statement': report['income_statement'],
                'metadata': report['metadata']
            }
        
        # å‰µå»ºæœå°‹ç´¢å¼•çµæ§‹
        search_index = {
            'index_info': {
                'title': 'ETFé‡é»æˆåˆ†è‚¡è²¡å ±æœå°‹ç´¢å¼•',
                'created_at': datetime.now().isoformat(),
                'total_companies': len(companies),
                'total_reports': len(self.stats['parsed_reports']),
                'data_source': 'TWSE',
                'coverage_period': f"{self.target_periods[0][0]}Q{self.target_periods[0][1]} - {self.target_periods[-1][0]}Q{self.target_periods[-1][1]}",
                'index_version': '3.0'
            },
            'companies': companies,
            'period_summary': {
                f"{year}Q{quarter}": {
                    'available_companies': [
                        r['stock_code'] for r in self.stats['parsed_reports'] 
                        if r['report_year'] == year and r['report_season'] == f"Q{quarter}"
                    ]
                }
                for year, quarter in self.target_periods
            },
            'company_summary': {
                stock_code: {
                    'company_name': data['company_name'],
                    'periods_count': len(data['periods']),
                    'available_periods': sorted(list(data['periods'].keys()))
                }
                for stock_code, data in companies.items()
            }
        }
        
        return search_index

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ ETFé‡é»æˆåˆ†è‚¡è²¡å ±å…¨é¢çˆ¬å–èˆ‡æ•´ç†ç³»çµ±")
    print("=" * 60)
    
    # å‰µå»ºçˆ¬èŸ²å¯¦ä¾‹
    crawler = ComprehensiveCrawler()
    
    print("ğŸ“‹ åŸ·è¡Œè¨ˆåŠƒ:")
    print(f"   ç›®æ¨™å…¬å¸: {list(crawler.target_companies.values())}")
    print(f"   ç›®æ¨™æœŸé–“: {crawler.target_periods[0][0]}Q{crawler.target_periods[0][1]} - {crawler.target_periods[-1][0]}Q{crawler.target_periods[-1][1]} (å…±{len(crawler.target_periods)}å­£)")
    print(f"   ç¸½ä»»å‹™æ•¸: {len(crawler.target_companies) * len(crawler.target_periods)}")
    
    # ç¢ºèªåŸ·è¡Œ
    confirm = input("\næ˜¯å¦ç¹¼çºŒåŸ·è¡Œ? (y/N): ").strip().lower()
    if confirm != 'y':
        print("âŒ å–æ¶ˆåŸ·è¡Œ")
        return
    
    # æ­¥é©Ÿ1: æ¸…ç†èˆŠè³‡æ–™
    crawler.cleanup_old_data()
    
    # æ­¥é©Ÿ2: å»ºç«‹æ–°çµæ§‹
    crawler.setup_new_structure()
    
    # æ­¥é©Ÿ3: åŸ·è¡Œå…¨é¢çˆ¬å–
    success = crawler.run_comprehensive_crawl()
    
    if success:
        print("\nâœ… å°ˆæ¡ˆæ•´ç†èˆ‡çˆ¬å–å®Œæˆ!")
        print(f"ğŸ“ çµæœå„²å­˜åœ¨: {crawler.main_results_dir}")
    else:
        print("\nâŒ çˆ¬å–éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œè«‹æª¢æŸ¥éŒ¯èª¤è¨Šæ¯")

if __name__ == '__main__':
    main()
