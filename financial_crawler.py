#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è²¡å ±çˆ¬èŸ²çµ±ä¸€ä»‹é¢
æ”¯æ´JSONè¼¸å…¥çš„å–®ç­†å’Œæ‰¹æ¬¡æŸ¥è©¢
"""

import json
import sys
from pathlib import Path
from datetime import datetime
import argparse

# æ·»åŠ scriptsè·¯å¾‘ä»¥ä¾¿å°å…¥æ¨¡çµ„
sys.path.append(str(Path(__file__).parent / "scripts"))

def load_config(config_path=None):
    """è¼‰å…¥é…ç½®æª”æ¡ˆ"""
    if config_path is None:
        config_path = Path(__file__).parent / "config" / "crawler_config.json"
    
    default_config = {
        "output_dir": "data/financial_reports",
        "test_output_dir": "data/test_results",
        "download_delay": 2,
        "max_retry": 3,
        "timeout": 30,
        "auto_validation": True,
        "generate_json": True
    }
    
    if config_path.exists():
        with open(config_path, 'r', encoding='utf-8') as f:
            config = json.load(f)
            # åˆä½µé è¨­é…ç½®
            default_config.update(config)
    
    return default_config

def download_company_report(stock_code, company_name, year, season, output_dir="data/financial_reports"):
    """
    é€šç”¨è²¡å ±ä¸‹è¼‰å‡½æ•¸
    åŸºæ–¼ improved_2330_test.py çš„é‚è¼¯æ”¹å¯«ç‚ºé€šç”¨ç‰ˆæœ¬
    """
    import requests
    import re
    from urllib.parse import urljoin
    
    print(f"ä¸‹è¼‰ {company_name}({stock_code}) {year}Q{season} è²¡å ±")
    
    # è¨­å®šè¼¸å‡ºç›®éŒ„
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)
    print(f"è¼¸å‡ºç›®éŒ„: {output_path}")
    
    # å»ºæ§‹PDFæª”æ¡ˆåç¨± (æ ¼å¼: YYYYMM_STOCKCODE_AI1.pdf)
    # å­£åº¦å°æ‡‰æœˆä»½: Q1=01, Q2=02, Q3=03, Q4=04
    month_map = {'1': '01', '2': '02', '3': '03', '4': '04'}
    season_str = str(season)
    
    pdf_year = year
    pdf_month = month_map[season_str]
    
    # ä½¿ç”¨å®Œæ•´å¹´ä»½
    filename = f"{pdf_year}{pdf_month}_{stock_code}_AI1.pdf"
    
    print(f"ç›®æ¨™æª”æ¡ˆ: {filename}")
    print(f"PDFå¹´ä»½: {pdf_year}, æœˆä»½: {pdf_month}")
    
    # TWSEç¶²ç«™åƒæ•¸
    base_url = "https://doc.twse.com.tw"
    query_url = f"{base_url}/server-java/t57sb01"
    
    query_data = {
        'step': '9',
        'kind': 'A',
        'co_id': stock_code,
        'filename': filename
    }
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Referer': f'{base_url}/server-java/t57sb01'
    }
    
    print(f"æŸ¥è©¢ URL: {query_url}")
    print(f"æŸ¥è©¢åƒæ•¸: {query_data}")
    
    try:
        session = requests.Session()
        
        # æ­¥é©Ÿ1ï¼šæŸ¥è©¢
        print("åŸ·è¡ŒæŸ¥è©¢...")
        response = session.post(query_url, data=query_data, headers=headers, timeout=30)
        
        print(f"å›æ‡‰ç‹€æ…‹ç¢¼: {response.status_code}")
        
        if response.status_code != 200:
            print(f"æŸ¥è©¢å¤±æ•—ï¼ŒHTTPç‹€æ…‹ç¢¼: {response.status_code}")
            return False
        
        # è§£æPDFé€£çµ
        content = response.text
        print(f"å›æ‡‰å…§å®¹é•·åº¦: {len(content)} å­—ç¬¦")
        
        # ä¿å­˜æŸ¥è©¢å›æ‡‰ä»¥ä¾›èª¿è©¦
        debug_dir = output_path / "debug"
        debug_dir.mkdir(exist_ok=True)
        debug_file = debug_dir / f"query_response_{stock_code}_{year}Q{season}.html"
        with open(debug_file, 'w', encoding='utf-8') as f:
            f.write(content)
        print(f"æŸ¥è©¢å›æ‡‰å·²ä¿å­˜: {debug_file}")
        
        pdf_link_pattern = r"href='(/pdf/[^']+)'"
        match = re.search(pdf_link_pattern, content)
        
        if not match:
            print("æœªæ‰¾åˆ°PDFé€£çµï¼Œå¯èƒ½è©²æœŸé–“ç„¡è²¡å ±è³‡æ–™")
            print("å˜—è©¦å°‹æ‰¾å…¶ä»–å¯èƒ½çš„é€£çµæ ¼å¼...")
            
            # å˜—è©¦å…¶ä»–å¯èƒ½çš„é€£çµæ ¼å¼
            alternative_patterns = [
                r"href=['\"]?(/pdf/[^'\">\s]+)",
                r"location\.href\s*=\s*['\"]?(/pdf/[^'\">\s]+)",
                r"window\.open\(['\"]?(/pdf/[^'\">\s]+)"
            ]
            
            for pattern in alternative_patterns:
                alt_match = re.search(pattern, content)
                if alt_match:
                    print(f"æ‰¾åˆ°æ›¿ä»£é€£çµæ ¼å¼: {alt_match.group(1)}")
                    match = alt_match
                    break
            
            if not match:
                return False
        
        pdf_path = match.group(1)
        pdf_url = urljoin(base_url, pdf_path)
        print(f"æ‰¾åˆ°PDFé€£çµ: {pdf_path}")
        print(f"å®Œæ•´PDF URL: {pdf_url}")
        
        # æ­¥é©Ÿ2ï¼šä¸‹è¼‰PDF
        print("ä¸‹è¼‰PDF...")
        pdf_headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/pdf,application/octet-stream,*/*',
            'Referer': query_url
        }
        
        pdf_response = session.get(pdf_url, headers=pdf_headers, stream=True, timeout=60)
        
        print(f"PDFå›æ‡‰ç‹€æ…‹ç¢¼: {pdf_response.status_code}")
        
        if pdf_response.status_code != 200:
            print(f"PDFä¸‹è¼‰å¤±æ•—ï¼ŒHTTPç‹€æ…‹ç¢¼: {pdf_response.status_code}")
            return False
        
        # æª¢æŸ¥å…§å®¹é¡å‹
        content_type = pdf_response.headers.get('Content-Type', '')
        print(f"å…§å®¹é¡å‹: {content_type}")
        
        if 'pdf' not in content_type.lower() and 'application/octet-stream' not in content_type:
            print(f"ä¸‹è¼‰çš„ä¸æ˜¯PDFæª”æ¡ˆ: {content_type}")
            return False
        
        # å„²å­˜PDFæª”æ¡ˆ
        file_path = output_path / filename
        downloaded_size = 0
        
        print(f"å„²å­˜åˆ°: {file_path}")
        
        with open(file_path, 'wb') as f:
            for chunk in pdf_response.iter_content(chunk_size=8192):
                if chunk:
                    f.write(chunk)
                    downloaded_size += len(chunk)
        
        actual_size = file_path.stat().st_size
        print(f"PDFä¸‹è¼‰å®Œæˆï¼")
        print(f"æª”æ¡ˆå¤§å°: {actual_size:,} bytes")
        
        # æª¢æŸ¥æª”æ¡ˆå®Œæ•´æ€§
        if actual_size < 10000:  # æª”æ¡ˆå¤ªå°å¯èƒ½æ˜¯éŒ¯èª¤é é¢
            print("æª”æ¡ˆå¤§å°ç•°å¸¸ï¼Œå¯èƒ½ä¸‹è¼‰ä¸å®Œæ•´")
            # æª¢æŸ¥æ˜¯å¦ç‚ºéŒ¯èª¤é é¢
            with open(file_path, 'rb') as f:
                first_bytes = f.read(100)
                if b'<html' in first_bytes.lower() or b'<body' in first_bytes.lower():
                    print("ä¸‹è¼‰çš„æ˜¯HTMLéŒ¯èª¤é é¢ï¼ŒéPDFæª”æ¡ˆ")
                    return False
            return False
        
        # ç”ŸæˆJSONæª”æ¡ˆ
        json_path = file_path.with_suffix('.json')
        json_data = {
            "stock_code": stock_code,
            "company_name": company_name,
            "report_year": year,
            "report_season": f"Q{season}",
            "currency": "TWD",
            "unit": "åƒå…ƒ",
            "financials": {
                "cash_and_equivalents": None,
                "accounts_receivable": None,
                "inventory": None,
                "total_assets": None,
                "total_liabilities": None,
                "equity": None
            },
            "income_statement": {
                "net_revenue": None,
                "gross_profit": None,
                "operating_income": None,
                "net_income": None,
                "eps": None
            },
            "metadata": {
                "source": "doc.twse.com.tw",
                "file_name": filename,
                "file_path": str(file_path),
                "file_size": actual_size,
                "download_url": pdf_url,
                "crawled_at": datetime.now().isoformat(),
                "parser_version": "unified_crawler_v1.0",
                "validation": {
                    "file_exists": file_path.exists(),
                    "size_reasonable": actual_size > 10000
                }
            }
        }
        
        with open(json_path, 'w', encoding='utf-8') as f:
            json.dump(json_data, f, ensure_ascii=False, indent=2)
        
        print(f"JSONæª”æ¡ˆå·²ç”Ÿæˆ: {json_path.name}")
        
        # æ›´æ–°ä¸»ç´¢å¼•
        add_to_master_index(stock_code, company_name, year, season, str(file_path), str(json_path), actual_size, success=True)
        
        return True
        
    except requests.exceptions.Timeout:
        print("ä¸‹è¼‰è¶…æ™‚")
        # è¨˜éŒ„å¤±æ•—åˆ°ä¸»ç´¢å¼•
        add_to_master_index(stock_code, company_name, year, season, None, None, 0, success=False)
        return False
    except Exception as e:
        print(f"ä¸‹è¼‰éç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        print(f"éŒ¯èª¤è©³æƒ…: {traceback.format_exc()}")
        # è¨˜éŒ„å¤±æ•—åˆ°ä¸»ç´¢å¼•
        add_to_master_index(stock_code, company_name, year, season, None, None, 0, success=False)
        return False

def process_single_query(query_data, config):
    """è™•ç†å–®ç­†æŸ¥è©¢"""
    print(f"è™•ç†å–®ç­†æŸ¥è©¢: {query_data.get('company_name', query_data.get('stock_code'))}")
    
    try:
        result = download_company_report(
            stock_code=query_data['stock_code'],
            company_name=query_data['company_name'],
            year=int(query_data['year']),
            season=str(query_data['season']).replace('Q', ''),
            output_dir=config['test_output_dir'] if query_data.get('test_mode', False) else config['output_dir']
        )
        
        return {
            "status": "success" if result else "failed",
            "stock_code": query_data['stock_code'],
            "company_name": query_data['company_name'],
            "period": f"{query_data['year']}Q{str(query_data['season']).replace('Q', '')}",
            "timestamp": datetime.now().isoformat(),
            "details": "ä¸‹è¼‰å®Œæˆ" if result else "ä¸‹è¼‰å¤±æ•—ï¼Œè«‹æª¢æŸ¥æœŸé–“æ˜¯å¦æ­£ç¢ºæˆ–ç¶²è·¯é€£ç·š"
        }
        
    except Exception as e:
        error_msg = str(e)
        print(f"è™•ç†éç¨‹ç™¼ç”ŸéŒ¯èª¤: {error_msg}")
        return {
            "status": "error",
            "error": error_msg,
            "stock_code": query_data['stock_code'],
            "company_name": query_data.get('company_name', 'æœªçŸ¥'),
            "period": f"{query_data.get('year', 'æœªçŸ¥')}Q{str(query_data.get('season', 'æœªçŸ¥')).replace('Q', '')}",
            "timestamp": datetime.now().isoformat()
        }


def process_batch_query(queries_data, config):
    """è™•ç†æ‰¹æ¬¡æŸ¥è©¢"""
    print(f"è™•ç†æ‰¹æ¬¡æŸ¥è©¢: {len(queries_data)} å€‹æŸ¥è©¢")
    
    results = []
    for i, query in enumerate(queries_data, 1):
        print(f"\n[{i}/{len(queries_data)}] è™•ç†: {query.get('company_name', query.get('stock_code'))}")
        result = process_single_query(query, config)
        results.append(result)
        
        # æ·»åŠ å»¶é²é¿å…è«‹æ±‚éå¿«
        if i < len(queries_data):
            import time
            time.sleep(config.get('download_delay', 2))
    
    return results

def validate_query_data(data):
    """é©—è­‰æŸ¥è©¢æ•¸æ“šæ ¼å¼"""
    required_fields = ['stock_code', 'company_name', 'year', 'season']
    
    if isinstance(data, list):
        # æ‰¹æ¬¡æŸ¥è©¢
        for i, item in enumerate(data):
            missing_fields = [field for field in required_fields if field not in item]
            if missing_fields:
                raise ValueError(f"æŸ¥è©¢ {i+1} ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
    else:
        # å–®ç­†æŸ¥è©¢
        missing_fields = [field for field in required_fields if field not in data]
        if missing_fields:
            raise ValueError(f"ç¼ºå°‘å¿…è¦æ¬„ä½: {missing_fields}")
    
    return True

def save_results(results, output_file=None):
    """å„²å­˜æŸ¥è©¢çµæœ"""
    if output_file is None:
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        output_file = f"query_results_{timestamp}.json"
    
    output_path = Path(__file__).parent / "output" / output_file
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(output_path, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"çµæœå·²å„²å­˜è‡³: {output_path}")
    return output_path

def load_master_index():
    """è¼‰å…¥ä¸»ç´¢å¼•æª”æ¡ˆ"""
    index_file = Path(__file__).parent / "data" / "master_index.json"
    
    if index_file.exists():
        try:
            with open(index_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"è­¦å‘Š: ä¸»ç´¢å¼•æª”æ¡ˆè®€å–éŒ¯èª¤: {e}")
            return {"version": "1.0", "last_updated": "", "total_reports": 0, "reports": []}
    else:
        return {"version": "1.0", "last_updated": "", "total_reports": 0, "reports": []}

def save_master_index(index_data):
    """å„²å­˜ä¸»ç´¢å¼•æª”æ¡ˆ"""
    index_file = Path(__file__).parent / "data" / "master_index.json"
    index_file.parent.mkdir(parents=True, exist_ok=True)
    
    # æ›´æ–°çµ±è¨ˆè³‡è¨Š
    index_data["last_updated"] = datetime.now().isoformat()
    index_data["total_reports"] = len(index_data["reports"])
    
    try:
        with open(index_file, 'w', encoding='utf-8') as f:
            json.dump(index_data, f, ensure_ascii=False, indent=2)
        print(f"ä¸»ç´¢å¼•å·²æ›´æ–°: {index_file}")
        return True
    except Exception as e:
        print(f"è­¦å‘Š: ä¸»ç´¢å¼•å„²å­˜å¤±æ•—: {e}")
        return False

def add_to_master_index(stock_code, company_name, year, season, pdf_path, json_path, file_size, success=True):
    """å°‡æ–°çš„è²¡å ±è¨˜éŒ„æ·»åŠ åˆ°ä¸»ç´¢å¼•"""
    index_data = load_master_index()
    
    # å»ºç«‹æ–°è¨˜éŒ„
    report_record = {
        "id": f"{stock_code}_{year}Q{season}",
        "stock_code": stock_code,
        "company_name": company_name,
        "year": year,
        "season": f"Q{season}",
        "period": f"{year}Q{season}",
        "pdf_file": str(pdf_path),
        "json_file": str(json_path),
        "file_size": file_size,
        "download_success": success,
        "crawled_at": datetime.now().isoformat(),
        "file_exists": Path(pdf_path).exists() if pdf_path else False
    }
    
    # æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨ç›¸åŒè¨˜éŒ„
    existing_index = -1
    for i, report in enumerate(index_data["reports"]):
        if report["id"] == report_record["id"]:
            existing_index = i
            break
    
    if existing_index >= 0:
        # æ›´æ–°ç¾æœ‰è¨˜éŒ„
        index_data["reports"][existing_index] = report_record
        print(f"æ›´æ–°ä¸»ç´¢å¼•è¨˜éŒ„: {report_record['id']}")
    else:
        # æ–°å¢è¨˜éŒ„
        index_data["reports"].append(report_record)
        print(f"æ–°å¢ä¸»ç´¢å¼•è¨˜éŒ„: {report_record['id']}")
    
    # æŒ‰æ—¥æœŸæ’åº (æœ€æ–°çš„åœ¨å‰)
    index_data["reports"].sort(key=lambda x: x["crawled_at"], reverse=True)
    
    return save_master_index(index_data)

def search_master_index(keyword=None, stock_code=None, company_name=None, year=None, season=None):
    """æœå°‹ä¸»ç´¢å¼•ä¸­çš„è²¡å ±è¨˜éŒ„"""
    index_data = load_master_index()
    results = []
    
    for report in index_data["reports"]:
        match = True
        
        # é—œéµå­—æœå°‹ (è‚¡ç¥¨ä»£ç¢¼æˆ–å…¬å¸åç¨±)
        if keyword:
            if keyword.lower() not in report["stock_code"].lower() and \
               keyword.lower() not in report["company_name"].lower():
                match = False
        
        # è‚¡ç¥¨ä»£ç¢¼ç²¾ç¢ºåŒ¹é…
        if stock_code and report["stock_code"] != stock_code:
            match = False
            
        # å…¬å¸åç¨±æ¨¡ç³ŠåŒ¹é…
        if company_name and company_name.lower() not in report["company_name"].lower():
            match = False
            
        # å¹´åº¦åŒ¹é…
        if year and report["year"] != int(year):
            match = False
            
        # å­£åº¦åŒ¹é…
        if season and report["season"] != f"Q{season}":
            match = False
        
        if match:
            results.append(report)
    
    return results

def show_master_index_stats():
    """é¡¯ç¤ºä¸»ç´¢å¼•çµ±è¨ˆè³‡è¨Š"""
    index_data = load_master_index()
    
    print("ğŸ“Š è²¡å ±ä¸»ç´¢å¼•çµ±è¨ˆ")
    print("=" * 40)
    print(f"ç¸½å ±å‘Šæ•¸: {index_data['total_reports']}")
    print(f"æœ€å¾Œæ›´æ–°: {index_data.get('last_updated', 'æœªçŸ¥')}")
    
    if index_data["reports"]:
        # çµ±è¨ˆå„å…¬å¸å ±å‘Šæ•¸é‡
        company_stats = {}
        year_stats = {}
        
        for report in index_data["reports"]:
            company = report["company_name"]
            year = str(report["year"])
            
            company_stats[company] = company_stats.get(company, 0) + 1
            year_stats[year] = year_stats.get(year, 0) + 1
        
        print(f"\nğŸ“ˆ å„å…¬å¸å ±å‘Šæ•¸:")
        for company, count in sorted(company_stats.items()):
            print(f"   {company}: {count} ä»½")
            
        print(f"\nğŸ“… å„å¹´åº¦å ±å‘Šæ•¸:")
        for year, count in sorted(year_stats.items(), reverse=True):
            print(f"   {year}: {count} ä»½")
        
        # æœ€æ–°ä¸‹è¼‰çš„5ç­†è¨˜éŒ„
        print(f"\nğŸ•’ æœ€æ–°ä¸‹è¼‰è¨˜éŒ„:")
        for i, report in enumerate(index_data["reports"][:5]):
            status = "âœ…" if report["download_success"] else "âŒ"
            print(f"   {i+1}. {report['company_name']}({report['stock_code']}) {report['period']} {status}")
    
    return index_data

def main():
    """ä¸»ç¨‹å¼"""
    parser = argparse.ArgumentParser(description='è²¡å ±çˆ¬èŸ²çµ±ä¸€ä»‹é¢')
    parser.add_argument('input', nargs='?', help='JSONè¼¸å…¥æª”æ¡ˆè·¯å¾‘æˆ–JSONå­—ä¸²')
    parser.add_argument('--config', help='é…ç½®æª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--output', help='çµæœè¼¸å‡ºæª”æ¡ˆè·¯å¾‘')
    parser.add_argument('--validate-only', action='store_true', help='åƒ…é©—è­‰è¼¸å…¥æ ¼å¼')
    parser.add_argument('--search', help='æœå°‹å·²ä¸‹è¼‰çš„è²¡å ± (é—œéµå­—)')
    parser.add_argument('--stock-code', help='æœå°‹æŒ‡å®šè‚¡ç¥¨ä»£ç¢¼')
    parser.add_argument('--company', help='æœå°‹æŒ‡å®šå…¬å¸åç¨±')
    parser.add_argument('--year', help='æœå°‹æŒ‡å®šå¹´åº¦')
    parser.add_argument('--season', help='æœå°‹æŒ‡å®šå­£åº¦')
    parser.add_argument('--stats', action='store_true', help='é¡¯ç¤ºä¸»ç´¢å¼•çµ±è¨ˆè³‡è¨Š')
    
    args = parser.parse_args()
    
    print("è²¡å ±çˆ¬èŸ²çµ±ä¸€ä»‹é¢")
    print("=" * 50)
    
    # å¦‚æœæ˜¯æœå°‹æ¨¡å¼
    if args.search or args.stock_code or args.company or args.year or args.season or args.stats:
        if args.stats:
            show_master_index_stats()
            return 0
        
        # åŸ·è¡Œæœå°‹
        results = search_master_index(
            keyword=args.search,
            stock_code=args.stock_code,
            company_name=args.company,
            year=args.year,
            season=args.season
        )
        
        if results:
            print(f"ğŸ” æœå°‹çµæœ: æ‰¾åˆ° {len(results)} ç­†è¨˜éŒ„")
            print("-" * 50)
            
            for i, report in enumerate(results, 1):
                status = "âœ…" if report["download_success"] and report["file_exists"] else "âŒ"
                size_mb = report["file_size"] / (1024*1024) if report["file_size"] > 0 else 0
                
                print(f"{i}. {report['company_name']}({report['stock_code']}) {report['period']} {status}")
                print(f"   æª”æ¡ˆ: {report['pdf_file']}")
                print(f"   å¤§å°: {size_mb:.1f}MB")
                print(f"   æ™‚é–“: {report['crawled_at'][:19]}")
                print()
                
        else:
            print("ğŸ” æœå°‹çµæœ: æœªæ‰¾åˆ°ç¬¦åˆæ¢ä»¶çš„è¨˜éŒ„")
        
        return 0
    
    # å¦‚æœæ²’æœ‰æä¾›è¼¸å…¥ï¼Œé¡¯ç¤ºå¹«åŠ©
    if not args.input:
        parser.print_help()
        print("\nğŸ“Š å¿«é€ŸæŸ¥çœ‹çµ±è¨ˆ: python financial_crawler.py --stats")
        print("ğŸ” æœå°‹ç¯„ä¾‹: python financial_crawler.py --search å°ç©é›»")
        return 1
    
    # è¼‰å…¥é…ç½®
    config = load_config(args.config)
    print("é…ç½®å·²è¼‰å…¥")
    
    # è§£æè¼¸å…¥
    try:
        if Path(args.input).exists():
            # å¾æª”æ¡ˆè®€å–
            with open(args.input, 'r', encoding='utf-8') as f:
                input_data = json.load(f)
            print(f"å¾æª”æ¡ˆè¼‰å…¥: {args.input}")
        else:
            # ç›´æ¥è§£æJSONå­—ä¸²
            input_data = json.loads(args.input)
            print(f"ç›´æ¥è§£æJSONå­—ä¸²")
    except (json.JSONDecodeError, FileNotFoundError) as e:
        print(f"è¼¸å…¥è§£æéŒ¯èª¤: {e}")
        return 1
    
    # é©—è­‰è¼¸å…¥æ ¼å¼
    try:
        validate_query_data(input_data)
        print("è¼¸inputæ ¼å¼é©—è­‰é€šé")
        
        if args.validate_only:
            print("åƒ…é©—è­‰æ¨¡å¼ï¼ŒçµæŸ")
            return 0
            
    except ValueError as e:
        print(f"è¼¸å…¥æ ¼å¼éŒ¯èª¤: {e}")
        return 1
    
    # è™•ç†æŸ¥è©¢
    start_time = datetime.now()
    
    if isinstance(input_data, list):
        # æ‰¹æ¬¡æŸ¥è©¢
        results = process_batch_query(input_data, config)
        query_type = "æ‰¹æ¬¡"
        query_count = len(input_data)
    else:
        # å–®ç­†æŸ¥è©¢
        result = process_single_query(input_data, config)
        results = [result]
        query_type = "å–®ç­†"
        query_count = 1
    
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()
    
    # çµ±è¨ˆçµæœ
    success_count = sum(1 for r in results if r.get('status') == 'success')
    failed_count = query_count - success_count
    
    print(f"\nåŸ·è¡Œçµæœ:")
    print(f"   æŸ¥è©¢é¡å‹: {query_type}")
    print(f"   ç¸½æŸ¥è©¢æ•¸: {query_count}")
    print(f"   æˆåŠŸ: {success_count}")
    print(f"   å¤±æ•—: {failed_count}")
    print(f"   è€—æ™‚: {duration:.1f} ç§’")
    
    # å„²å­˜çµæœ
    output_path = save_results(results, args.output)
    
    print(f"\næŸ¥è©¢å®Œæˆï¼")
    print(f"è©³ç´°çµæœè«‹æŸ¥çœ‹: {output_path}")
    
    return 0 if failed_count == 0 else 1

if __name__ == '__main__':
    sys.exit(main())
