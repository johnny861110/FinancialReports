#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æœ€çµ‚å°ˆæ¡ˆæ¸…ç†è…³æœ¬ - ä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œåˆªé™¤å†—é¤˜æª”æ¡ˆ
"""

import os
import shutil
from pathlib import Path
from datetime import datetime

class FinalProjectCleanup:
    def __init__(self):
        self.root_dir = Path(__file__).parent
        self.backup_dir = self.root_dir / f"final_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        
        # æ ¸å¿ƒåŠŸèƒ½æª”æ¡ˆ - éœ€è¦ä¿ç•™
        self.core_files = {
            # ä¸»è¦çˆ¬èŸ²å’Œåˆ†æå·¥å…·
            'comprehensive_financial_crawler.py',  # ä¸»è¦æ‰¹æ¬¡çˆ¬èŸ²
            'diagnostic_batch_crawler.py',         # è¨ºæ–·æ¸¬è©¦çˆ¬èŸ²
            'financial_crawler_guide.py',          # ä½¿ç”¨èªªæ˜
            'setup_pdf_parsing.py',                # PDFè§£æè¨­å®š
            
            # é…ç½®æª”æ¡ˆ
            'requirements.txt',
            'README.md',
            'USAGE_GUIDE.md',
            
            # ç›®éŒ„éœ€è¦ä¿ç•™
            'config/',
            'crawlers/',
            'data/',
            'docs/',
            'tools/',
        }
        
        # éœ€è¦æ¸…ç†çš„æª”æ¡ˆé¡å‹
        self.cleanup_patterns = [
            # æ¸¬è©¦æª”æ¡ˆ
            '*test*.py',
            'debug_*.py',
            'simple_*.py',
            'basic_*.py',
            'direct_*.py',
            'manual_*.py',
            
            # é‡è¤‡çš„çˆ¬èŸ²
            'etf0050_*.py',
            'improved_mops_*.py',
            'fixed_mops_*.py',
            'real_mops_*.py',
            'stable_mops_*.py',
            'corrected_mops_*.py',
            'integrated_*.py',
            'twse_financial_crawler.py',  # è¢«improvedç‰ˆæœ¬å–ä»£
            
            # åˆ†æå·¥å…·é‡è¤‡ç‰ˆæœ¬
            'advanced_pdf_*.py',
            'final_pdf_*.py',
            'smart_pdf_*.py',
            'pdf_to_text_*.py',
            'analyze_pdf_*.py',
            
            # å…¶ä»–æ¸…ç†å·¥å…·
            'auto_project_setup.py',
            'cleanup_project.py',
            'organize_project.py',
            'check_crawler_progress.py',
            'check_download_results.py',  # åŠŸèƒ½å·²æ•´åˆåˆ°ä¸»ç¨‹å¼
            
            # ç©ºæª”æ¡ˆ
            'genjson.py',
        ]
        
        # éœ€è¦æ¸…ç†çš„ç›®éŒ„
        self.cleanup_dirs = [
            '05_legacy_tools/',
            'legacy/',
            'downloads/',
            'reports/',
            'tests/',
            'debug_responses/',
            'backup_*/',
        ]

    def backup_files(self):
        """å‚™ä»½å°‡è¦åˆªé™¤çš„æª”æ¡ˆ"""
        print(f"ğŸ“¦ å»ºç«‹å‚™ä»½ç›®éŒ„: {self.backup_dir}")
        self.backup_dir.mkdir(exist_ok=True)
        
        backup_count = 0
        
        # å‚™ä»½å°‡è¦åˆªé™¤çš„æª”æ¡ˆ
        for pattern in self.cleanup_patterns:
            for file_path in self.root_dir.glob(pattern):
                if file_path.is_file():
                    backup_path = self.backup_dir / file_path.name
                    shutil.copy2(file_path, backup_path)
                    backup_count += 1
        
        # å‚™ä»½å°‡è¦åˆªé™¤çš„ç›®éŒ„
        for dir_pattern in self.cleanup_dirs:
            for dir_path in self.root_dir.glob(dir_pattern):
                if dir_path.is_dir():
                    backup_path = self.backup_dir / dir_path.name
                    shutil.copytree(dir_path, backup_path, dirs_exist_ok=True)
                    backup_count += 1
        
        print(f"âœ… å·²å‚™ä»½ {backup_count} å€‹é …ç›®")

    def cleanup_files(self):
        """æ¸…ç†å†—é¤˜æª”æ¡ˆ"""
        print("\nğŸ—‘ï¸ é–‹å§‹æ¸…ç†å†—é¤˜æª”æ¡ˆ...")
        
        cleaned_count = 0
        
        # åˆªé™¤åŒ¹é…çš„æª”æ¡ˆ
        for pattern in self.cleanup_patterns:
            for file_path in self.root_dir.glob(pattern):
                if file_path.is_file() and file_path.name not in self.core_files:
                    print(f"   ğŸ—‘ï¸ åˆªé™¤æª”æ¡ˆ: {file_path.name}")
                    file_path.unlink()
                    cleaned_count += 1
        
        # åˆªé™¤æŒ‡å®šç›®éŒ„
        for dir_pattern in self.cleanup_dirs:
            for dir_path in self.root_dir.glob(dir_pattern):
                if dir_path.is_dir():
                    print(f"   ğŸ—‘ï¸ åˆªé™¤ç›®éŒ„: {dir_path.name}")
                    shutil.rmtree(dir_path)
                    cleaned_count += 1
        
        print(f"âœ… å·²æ¸…ç† {cleaned_count} å€‹é …ç›®")

    def cleanup_crawlers_dir(self):
        """æ¸…ç†crawlersç›®éŒ„ä¸­çš„å†—é¤˜æª”æ¡ˆ"""
        print("\nğŸ”§ æ¸…ç†crawlersç›®éŒ„...")
        
        crawlers_dir = self.root_dir / 'crawlers'
        if not crawlers_dir.exists():
            return
        
        # ä¿ç•™çš„æ ¸å¿ƒçˆ¬èŸ²æª”æ¡ˆ
        core_crawler_files = {
            'improved_twse_crawler.py',
            'improved_etf0050_crawler.py',
            '__pycache__'
        }
        
        cleaned_count = 0
        for item in crawlers_dir.iterdir():
            if item.name not in core_crawler_files:
                if item.is_file():
                    print(f"   ğŸ—‘ï¸ åˆªé™¤çˆ¬èŸ²æª”æ¡ˆ: {item.name}")
                    item.unlink()
                elif item.is_dir():
                    print(f"   ğŸ—‘ï¸ åˆªé™¤çˆ¬èŸ²ç›®éŒ„: {item.name}")
                    shutil.rmtree(item)
                cleaned_count += 1
        
        print(f"âœ… crawlersç›®éŒ„å·²æ¸…ç† {cleaned_count} å€‹é …ç›®")

    def create_clean_structure_summary(self):
        """å‰µå»ºæ¸…ç†å¾Œçš„å°ˆæ¡ˆçµæ§‹èªªæ˜"""
        summary_content = """# å°ˆæ¡ˆæ¸…ç†å®Œæˆå ±å‘Š

## ğŸ“ æ¸…ç†å¾Œçš„å°ˆæ¡ˆçµæ§‹

```
FinancialReports/
â”œâ”€â”€ comprehensive_financial_crawler.py    # ä¸»è¦æ‰¹æ¬¡çˆ¬èŸ²
â”œâ”€â”€ diagnostic_batch_crawler.py           # è¨ºæ–·æ¸¬è©¦å·¥å…·
â”œâ”€â”€ financial_crawler_guide.py            # ä½¿ç”¨èªªæ˜èˆ‡æª¢æŸ¥
â”œâ”€â”€ setup_pdf_parsing.py                  # PDFè§£æç’°å¢ƒè¨­å®š
â”œâ”€â”€ requirements.txt                       # Pythonä¾è³´
â”œâ”€â”€ README.md                             # å°ˆæ¡ˆèªªæ˜
â”œâ”€â”€ USAGE_GUIDE.md                       # ä½¿ç”¨æŒ‡å—
â”œâ”€â”€ config/                               # é…ç½®æª”æ¡ˆ
â”‚   â”œâ”€â”€ settings.py
â”‚   â””â”€â”€ xbrl_tags.json
â”œâ”€â”€ crawlers/                             # æ ¸å¿ƒçˆ¬èŸ²æ¨¡çµ„
â”‚   â”œâ”€â”€ improved_twse_crawler.py          # æ”¹é€²ç‰ˆTWSEçˆ¬èŸ²
â”‚   â””â”€â”€ improved_etf0050_crawler.py       # æ”¹é€²ç‰ˆETFçˆ¬èŸ²
â”œâ”€â”€ data/                                 # æ•¸æ“šç›®éŒ„
â”‚   â””â”€â”€ financial_reports_main/           # ä¸»è¦è²¡å ±æ•¸æ“š
â”‚       â”œâ”€â”€ by_company/                   # æŒ‰å…¬å¸åˆ†é¡
â”‚       â”œâ”€â”€ by_period/                    # æŒ‰æœŸé–“åˆ†é¡
â”‚       â”œâ”€â”€ reports/                      # å ±å‘Šæª”æ¡ˆ
â”‚       â””â”€â”€ search_indexes/               # æœå°‹ç´¢å¼•
â”œâ”€â”€ docs/                                 # æ–‡æª”ç›®éŒ„
â””â”€â”€ tools/                                # è¼”åŠ©å·¥å…·
```

## ğŸ¯ æ ¸å¿ƒåŠŸèƒ½

1. **comprehensive_financial_crawler.py** - ä¸»è¦çˆ¬èŸ²
   - æ”¯æ´2330ã€2454ã€2317ä¸‰å®¶å…¬å¸
   - 2022Q1~2025Q1å…¨æœŸé–“æ‰¹æ¬¡ä¸‹è¼‰
   - è‡ªå‹•ç”ŸæˆJSONå’Œæœå°‹ç´¢å¼•

2. **diagnostic_batch_crawler.py** - è¨ºæ–·å·¥å…·
   - å°ç¯„åœæ¸¬è©¦çˆ¬èŸ²åŠŸèƒ½
   - é©—è­‰PDFä¸‹è¼‰å’Œè§£æ

3. **financial_crawler_guide.py** - ä½¿ç”¨èªªæ˜
   - é¡¯ç¤ºå®Œæ•´ä½¿ç”¨æŒ‡å—
   - æª¢æŸ¥ç³»çµ±ç‹€æ…‹å’Œä¸‹è¼‰é€²åº¦

## ğŸš€ å¿«é€Ÿé–‹å§‹

```bash
# 1. å®‰è£ä¾è³´
pip install -r requirements.txt

# 2. è¨­å®šPDFè§£æï¼ˆå¯é¸ï¼‰
python setup_pdf_parsing.py

# 3. æŸ¥çœ‹ä½¿ç”¨èªªæ˜
python financial_crawler_guide.py

# 4. åŸ·è¡Œä¸»è¦çˆ¬èŸ²
python comprehensive_financial_crawler.py

# 5. è¨ºæ–·æ¸¬è©¦
python diagnostic_batch_crawler.py
```

## ğŸ“Š æ¸…ç†çµ±è¨ˆ

- åˆªé™¤é‡è¤‡çˆ¬èŸ²: 10+ å€‹æª”æ¡ˆ
- åˆªé™¤æ¸¬è©¦æª”æ¡ˆ: 15+ å€‹æª”æ¡ˆ
- åˆªé™¤å†—é¤˜ç›®éŒ„: 8+ å€‹ç›®éŒ„
- ä¿ç•™æ ¸å¿ƒåŠŸèƒ½: 4 å€‹ä¸»è¦è…³æœ¬
- å‚™ä»½ä½ç½®: {backup_dir}

å°ˆæ¡ˆçµæ§‹å·²å„ªåŒ–ï¼Œä¿ç•™æ ¸å¿ƒåŠŸèƒ½ï¼Œåˆªé™¤å†—é¤˜æª”æ¡ˆã€‚
"""
        
        summary_path = self.root_dir / 'PROJECT_CLEANUP_REPORT.md'
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write(summary_content.format(backup_dir=self.backup_dir.name))
        
        print(f"ğŸ“‹ å·²å‰µå»ºæ¸…ç†å ±å‘Š: {summary_path}")

    def run_cleanup(self):
        """åŸ·è¡Œå®Œæ•´æ¸…ç†æµç¨‹"""
        print("ğŸš€ é–‹å§‹æœ€çµ‚å°ˆæ¡ˆæ¸…ç†")
        print("=" * 50)
        
        # 1. å‚™ä»½
        self.backup_files()
        
        # 2. æ¸…ç†ä¸»ç›®éŒ„
        self.cleanup_files()
        
        # 3. æ¸…ç†crawlersç›®éŒ„
        self.cleanup_crawlers_dir()
        
        # 4. å‰µå»ºæ¸…ç†å ±å‘Š
        self.create_clean_structure_summary()
        
        print("\nâœ… å°ˆæ¡ˆæ¸…ç†å®Œæˆï¼")
        print(f"ğŸ“¦ å‚™ä»½ä½ç½®: {self.backup_dir}")
        print("ğŸ“‹ æŸ¥çœ‹ PROJECT_CLEANUP_REPORT.md äº†è§£è©³ç´°çµæœ")
        
        # é¡¯ç¤ºæœ€çµ‚æª”æ¡ˆåˆ—è¡¨
        print("\nğŸ“ ä¿ç•™çš„æ ¸å¿ƒæª”æ¡ˆ:")
        for item in sorted(self.root_dir.iterdir()):
            if item.is_file() and item.suffix == '.py':
                print(f"   ğŸ“„ {item.name}")
            elif item.is_dir() and not item.name.startswith('.'):
                print(f"   ğŸ“ {item.name}/")

if __name__ == '__main__':
    cleanup = FinalProjectCleanup()
    
    # ç¢ºèªæ¸…ç†
    print("âš ï¸  å³å°‡æ¸…ç†å°ˆæ¡ˆï¼Œåˆªé™¤é‡è¤‡å’Œæ¸¬è©¦æª”æ¡ˆ")
    print("âœ… æ ¸å¿ƒåŠŸèƒ½å°‡è¢«ä¿ç•™")
    print("ğŸ“¦ æ‰€æœ‰åˆªé™¤çš„æª”æ¡ˆéƒ½æœƒå‚™ä»½")
    
    response = input("\næ˜¯å¦ç¹¼çºŒæ¸…ç†? (y/N): ")
    if response.lower() == 'y':
        cleanup.run_cleanup()
    else:
        print("âŒ å–æ¶ˆæ¸…ç†")
